{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import glob\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install cvxpy for L1 norm minimization for PeriodStrength fun (Ramanujan methods)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import torchvision.transforms as T\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from six.moves import xrange\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from ipynb.fs.full.WaveSeparatorCode import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Diffusion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, device, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "\n",
    "    pl_sd = torch.load(ckpt, map_location=device)\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_im(im_path):\n",
    "    if im_path.startswith(\"http\"):\n",
    "        response = requests.get(im_path)\n",
    "        response.raise_for_status()\n",
    "        im = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        im = Image.open(im_path).convert(\"RGB\")\n",
    "    tforms = transforms.Compose([\n",
    "        # transforms.Resize(224),\n",
    "        # transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    inp = tforms(im).unsqueeze(0)\n",
    "    return inp*2-1\n",
    "\n",
    "def prep_im(img):\n",
    "    \n",
    "    tforms = transforms.Compose([\n",
    "        # transforms.Resize(224),\n",
    "        # transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    inp = tforms(img).unsqueeze(0)\n",
    "    return inp*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_model(input_im, model, sampler, precision, h, w, ddim_steps, n_samples, scale, ddim_eta):\n",
    "    precision_scope = autocast if precision==\"autocast\" else nullcontext\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            c = model.get_learned_conditioning(input_im).tile(n_samples,1,1)\n",
    "\n",
    "            if scale != 1.0:\n",
    "                uc = torch.zeros_like(c)\n",
    "            else:\n",
    "                uc = None\n",
    "\n",
    "            shape = [4, h // 8, w // 8]\n",
    "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=n_samples,\n",
    "                                             shape=shape,\n",
    "                                             verbose=False,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             unconditional_conditioning=uc,\n",
    "                                             eta=ddim_eta,\n",
    "                                             x_T=None)\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            return torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(\n",
    "    im_path=\"data/example_conditioning/superresolution/sample_0.jpg\",\n",
    "    ckpt=\"models/ldm/stable-diffusion-v1/sd-clip-vit-l14-img-embed_ema_only.ckpt\",\n",
    "    config=\"configs/stable-diffusion/sd-image-condition-finetune.yaml\",\n",
    "    outpath=\"im_variations\",\n",
    "    scale=3.0,\n",
    "    h=512,\n",
    "    w=512,\n",
    "    n_samples=4,\n",
    "    precision=\"fp32\",\n",
    "    plms=True,\n",
    "    ddim_steps=50,\n",
    "    ddim_eta=0.0,\n",
    "    device_idx=0,\n",
    "    save=True,\n",
    "    eval=True,\n",
    "    ):\n",
    "\n",
    "    device = f\"cuda:{device_idx}\"\n",
    "    config = OmegaConf.load(config)\n",
    "    model = load_model_from_config(config, ckpt, device=device)\n",
    "\n",
    "    if plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "        ddim_eta = 0.0\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    if isinstance(im_path, str):\n",
    "        im_paths = glob.glob(im_path)\n",
    "    im_paths = sorted(im_paths)\n",
    "\n",
    "    all_similarities = []\n",
    "\n",
    "    for im in im_paths:\n",
    "        input_im = load_im(im).to(device)\n",
    "\n",
    "        x_samples_ddim = sample_model(input_im, model, sampler, precision, h, w, ddim_steps, n_samples, scale, ddim_eta)\n",
    "        if save:\n",
    "            for x_sample in x_samples_ddim:\n",
    "                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                filename = os.path.join(sample_path, f\"{base_count:05}.png\")\n",
    "                Image.fromarray(x_sample.astype(np.uint8)).save(filename)\n",
    "                base_count += 1\n",
    "\n",
    "        if eval:\n",
    "            generated_embed = model.get_learned_conditioning(x_samples_ddim).squeeze(1)\n",
    "            prompt_embed = model.get_learned_conditioning(input_im).squeeze(1)\n",
    "\n",
    "            generated_embed /= generated_embed.norm(dim=-1, keepdim=True)\n",
    "            prompt_embed /= prompt_embed.norm(dim=-1, keepdim=True)\n",
    "            similarity = prompt_embed @ generated_embed.T\n",
    "            mean_sim = similarity.mean()\n",
    "            all_similarities.append(mean_sim.unsqueeze(0))\n",
    "\n",
    "    df = pd.DataFrame(zip(im_paths, [x.item() for x in all_similarities]), columns=[\"filename\", \"similarity\"])\n",
    "    df.to_csv(os.path.join(sample_path, \"eval.csv\"))\n",
    "    print(torch.cat(all_similarities).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/stable-diffusion/sd-image-condition-finetune.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     fire\u001b[39m.\u001b[39;49mFire(main)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fire\\core.py:141\u001b[0m, in \u001b[0;36mFire\u001b[1;34m(component, command, name)\u001b[0m\n\u001b[0;32m    138\u001b[0m   context\u001b[39m.\u001b[39mupdate(caller_globals)\n\u001b[0;32m    139\u001b[0m   context\u001b[39m.\u001b[39mupdate(caller_locals)\n\u001b[1;32m--> 141\u001b[0m component_trace \u001b[39m=\u001b[39m _Fire(component, args, parsed_flag_args, context, name)\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m component_trace\u001b[39m.\u001b[39mHasError():\n\u001b[0;32m    144\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fire\\core.py:466\u001b[0m, in \u001b[0;36m_Fire\u001b[1;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[0;32m    463\u001b[0m is_class \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39misclass(component)\n\u001b[0;32m    465\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m   component, remaining_args \u001b[39m=\u001b[39m _CallAndUpdateTrace(\n\u001b[0;32m    467\u001b[0m       component,\n\u001b[0;32m    468\u001b[0m       remaining_args,\n\u001b[0;32m    469\u001b[0m       component_trace,\n\u001b[0;32m    470\u001b[0m       treatment\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m is_class \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mroutine\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    471\u001b[0m       target\u001b[39m=\u001b[39;49mcomponent\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m    472\u001b[0m   handled \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39mexcept\u001b[39;00m FireError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fire\\core.py:681\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[1;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[0;32m    679\u001b[0m   component \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39mrun_until_complete(fn(\u001b[39m*\u001b[39mvarargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    680\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m   component \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39mvarargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m treatment \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    684\u001b[0m   action \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mINSTANTIATED_CLASS\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(im_path, ckpt, config, outpath, scale, h, w, n_samples, precision, plms, ddim_steps, ddim_eta, device_idx, save, eval)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(\n\u001b[0;32m      2\u001b[0m     im_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/example_conditioning/superresolution/sample_0.jpg\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     ckpt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/ldm/stable-diffusion-v1/sd-clip-vit-l14-img-embed_ema_only.ckpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39meval\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m     ):\n\u001b[0;32m     19\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mdevice_idx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     config \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39;49mload(config)\n\u001b[0;32m     21\u001b[0m     model \u001b[39m=\u001b[39m load_model_from_config(config, ckpt, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m plms:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\omegaconf\\omegaconf.py:187\u001b[0m, in \u001b[0;36mOmegaConf.load\u001b[1;34m(file_)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_yaml_loader\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(file_, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath)):\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39;49mopen(file_, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    188\u001b[0m         obj \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39mload(f, Loader\u001b[39m=\u001b[39mget_yaml_loader())\n\u001b[0;32m    189\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mgetattr\u001b[39m(file_, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'configs/stable-diffusion/sd-image-condition-finetune.yaml'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_root = r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 available GPU(s)\n",
      "GPU 0: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Found {torch.cuda.device_count()} available GPU(s)\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\\models/ldm/stable-diffusion-v1/sd-clip-vit-l14-img-embed_ema_only.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.39 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m ckpt \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(sd_root, ckpt)\n\u001b[0;32m      5\u001b[0m config\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigs/stable-diffusion/sd-image-condition-finetune.yaml\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m load_model_from_config(config, ckpt, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m      7\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))[\u001b[39m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m c \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_learned_conditioning(image)\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, ckpt, device, verbose)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_from_config\u001b[39m(config, ckpt, device, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading model from \u001b[39m\u001b[39m{\u001b[39;00mckpt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     pl_sd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(ckpt, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m pl_sd:\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGlobal Step: \u001b[39m\u001b[39m{\u001b[39;00mpl_sd[\u001b[39m'\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[0;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[1;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:970\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, map_location)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_UntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[0;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.39 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device_idx=0\n",
    "device = f\"cuda:{device_idx}\"\n",
    "ckpt=\"models/ldm/stable-diffusion-v1/sd-clip-vit-l14-img-embed_ema_only.ckpt\"\n",
    "ckpt = os.path.join(sd_root, ckpt)\n",
    "config=\"configs/stable-diffusion/sd-image-condition-finetune.yaml\"\n",
    "model = load_model_from_config(config, ckpt, device=device)\n",
    "image = next(iter(train_loader))[0]\n",
    "c = model.get_learned_conditioning(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = (r\"C:\\Users\\bruno\\OneDrive\\Desktop\\Psychology\\For Thesis Project\\Datasets\\My_ImageNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportImagenet(Dataset):\n",
    "    def __init__ (self, root_dir, mode, model):\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        downed_image= None\n",
    "        if mode == 'Test':\n",
    "            fileid = 'Test_list.csv'\n",
    "        elif mode == 'Val':\n",
    "            fileid = 'Val_list.csv'\n",
    "        elif mode == 'Train':\n",
    "            fileid = 'Train_list.csv'\n",
    "        self.csv = pd.read_csv(os.path.join(self.root_dir,fileid), delimiter= ',')       \n",
    "        self.targets = torch.cuda.IntTensor(self.csv[\"Target\"]).long()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        CLIP = False\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[index,0])\n",
    "        \n",
    "        #This is used when NOT preprocessing images for CLIP\n",
    "        if(not CLIP):\n",
    "\n",
    "            image = io.imread(img_path)\n",
    "            image = T.ToTensor()(image)\n",
    "            image = T.CenterCrop((200, 200))(image)\n",
    "            image = T.Resize(192)(image)\n",
    "            # y_label = self.targets[index] \n",
    "\n",
    "            return(image.to(device)) \n",
    "\n",
    "        else:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                image_token = model.get_learned_conditioning(image)\n",
    "                # image_token = image_token.squeeze(1)\n",
    "        \n",
    "\n",
    "            return(image.to(device), image_token.to(device))\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m subset_cut \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      4\u001b[0m shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data_set \u001b[39m=\u001b[39m ImportImagenet(root_dir, \u001b[39m'\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m dataset_len\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_set)\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m (subset \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m    \n\u001b[0;32m     14\u001b[0m     \u001b[39m# Subset dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36mImportImagenet.__init__\u001b[1;34m(self, root_dir, mode)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m (\u001b[39mself\u001b[39m, root_dir, mode):\n\u001b[0;32m      3\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir \u001b[39m=\u001b[39m root_dir\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m      5\u001b[0m     downed_image\u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "batch = 5\n",
    "subset = True\n",
    "subset_cut = 10\n",
    "shuffle = True\n",
    "\n",
    "device_idx=0\n",
    "device = f\"cuda:{device_idx}\"\n",
    "ckpt=\"models/ldm/stable-diffusion-v1/sd-clip-vit-l14-img-embed_ema_only.ckpt\"\n",
    "ckpt = os.path.join(sd_root, ckpt)\n",
    "config=\"configs/stable-diffusion/sd-image-condition-finetune.yaml\"\n",
    "model = load_model_from_config(config, ckpt, device=device)\n",
    "\n",
    "data_set = ImportImagenet(root_dir, 'Train', model)\n",
    "dataset_len= len(data_set)\n",
    "\n",
    "if (subset == True):\n",
    "\n",
    "   \n",
    "    # Subset dataset\n",
    "    data_set = torch.utils.data.Subset(data_set, np.arange(0,len(data_set),subset_cut))\n",
    "    \n",
    "    dataset_len = len(data_set)\n",
    "    \n",
    "    val_split = int(0.1 * dataset_len)\n",
    "    test_split = int(0.1 * dataset_len)\n",
    "    train_split = dataset_len - val_split - test_split\n",
    "\n",
    "else:\n",
    "    \n",
    "    val_split = int(0.1 * dataset_len)\n",
    "    test_split = int(0.1 * dataset_len)\n",
    "    train_split = dataset_len - val_split - test_split\n",
    "\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data_set, [train_split, val_split, test_split])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)\n",
    "test_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize images (they appear distorted when processed for clip)\n",
    "\n",
    "images = next(iter(train_loader))\n",
    "\n",
    "for image in images[0].squeeze(dim=1):\n",
    "   \n",
    "    plt.imshow(T.ToPILImage()(image.squeeze()))\n",
    "    plt.figure()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for a single electrode (linear regression)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 124 electrodes\n",
    "\n",
    "num_electrodes = 124\n",
    "input_size = 150528 # Flattened image size\n",
    "output_size = 1\n",
    "electrodes = nn.ModuleList([LinearRegression(input_size, output_size).to(device) for _ in range(num_electrodes)])\n",
    "\n",
    "# Pass the image through each electrode and collect the outputs\n",
    "\n",
    "def readBrain(images, electrodes = electrodes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        brain_data = []\n",
    "        for image in images: \n",
    "            electrode_data = []\n",
    "            for electrode in electrodes:\n",
    "                electrode_data.append(electrode(image.flatten()))\n",
    "\n",
    "            # Concatenate the outputs into a single tensor\n",
    "            electrodes_data = torch.cat(electrode_data, dim=0)\n",
    "\n",
    "            brain_data.append(electrodes_data)\n",
    "          \n",
    "\n",
    "        brain_data = torch.stack(brain_data, dim=0)\n",
    "\n",
    "        n_brain_data = brain_data + (0.1**0.5)*torch.randn(brain_data.shape).to(device)\n",
    "    \n",
    "        return n_brain_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing Latents from simulated electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructor = LinearRegression(124, 512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "\n",
    "    def __init__(self, max_epoch, learning_rate = 0.01, network = reconstructor):\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.best_vloss = None\n",
    "        self.bad_validation_counter = 0\n",
    "        self.val_stop = False\n",
    "        self.saved_parameters1 = None\n",
    "        self.max_epoch = max_epoch\n",
    "        self.epoch_stop = False\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.Adam(reconstructor.parameters(), lr=learning_rate)\n",
    "        self.network = network\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for layer in self.network.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "\n",
    "    def val_early_stopping(self,  vloss, network, patience):\n",
    "    \n",
    "\n",
    "        if self.best_vloss == None:\n",
    "            self.best_vloss = vloss\n",
    "\n",
    "        elif self.best_vloss > vloss:      #if best_vloss > vloss  \n",
    "            self.bad_validation_counter = 0\n",
    "            self.best_vloss = vloss\n",
    "            \n",
    "        elif self.best_vloss <= vloss:      #if best_vloss <= vloss\n",
    "            self.bad_validation_counter += 1\n",
    "            \n",
    "            if self.bad_validation_counter == 1:\n",
    "                self.saved_parameters1 = network.state_dict() \n",
    "                        \n",
    "# Stop if validation performance does not improve for patience number of epochs\n",
    "        if self.bad_validation_counter >= patience:\n",
    "            self.val_stop = True\n",
    "            print(\"Val stop\")\n",
    "\n",
    "    def train_track_loss(self, loss, batch):\n",
    "        self.train_epoch_loss += loss.item()\n",
    "        \n",
    "    def val_track_loss(self, loss, batch):\n",
    "        self.val_epoch_loss += loss.item()\n",
    "\n",
    "    def test_track_loss(self, loss, batch):\n",
    "        self.test_epoch_loss += loss.item()\n",
    "\n",
    "    def begin_epoch(self):  \n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.train_epoch_loss = 0\n",
    "        self.test_epoch_loss = 0\n",
    "        self.val_epoch_loss = 0\n",
    "\n",
    "    # Stop if max_epoch is reached\n",
    "        if self.max_epoch != None:\n",
    "            if self.epoch_count == self.max_epoch:\n",
    "                self.epoch_stop = True\n",
    "\n",
    "\n",
    "    def networkStep(self, network, loader, mode):\n",
    "\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in loader:\n",
    "\n",
    "                n_batches = n_batches + 1\n",
    "\n",
    "                images, tokens = batch\n",
    "                tokens = tokens.squeeze(1)\n",
    "                brain_data = readBrain(images)\n",
    "                if(mode == \"Train\"):\n",
    "                    self.optimizer.zero_grad();\n",
    "                output = network(brain_data)\n",
    "                loss = self.loss_function(output, tokens)\n",
    "\n",
    "                if(mode == \"Train\"):\n",
    "             \n",
    "                    loss.backward();\n",
    "                    self.optimizer.step();\n",
    "\n",
    "                    self.train_track_loss(loss,batch)\n",
    "\n",
    "                elif(mode == \"Val\"):\n",
    "                  \n",
    "                    self.val_track_loss(loss,batch)\n",
    "                else:\n",
    "                    self.test_track_loss(loss,batch)\n",
    "\n",
    "        return n_batches\n",
    "\n",
    "    def train_one_epoch(self, network, train_loader):\n",
    "\n",
    "        network.train()\n",
    "        last_loss = 0         \n",
    "        n_batches = self.networkStep(network, train_loader, \"Train\")                  \n",
    "        last_loss = self.train_epoch_loss / n_batches \n",
    "        \n",
    "        return last_loss\n",
    "\n",
    "    def val_one_epoch(self, network, val_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_vloss = 0\n",
    "            n_batches = self.networkStep(network, val_loader, \"Val\")     \n",
    "            last_vloss = self.val_epoch_loss / n_batches \n",
    "    \n",
    "        return last_vloss\n",
    "\n",
    "    def test_run(self, network, test_loader):\n",
    "        self.begin_epoch()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_tloss = 0\n",
    "            n_batches = self.networkStep(network, test_loader, \"Test\")\n",
    "            last_tloss = self.test_epoch_loss / n_batches \n",
    "\n",
    "        return last_tloss\n",
    "    \n",
    "    \n",
    "    def train_val_run(self, network, train_loader, val_loader, val_patience = 3):\n",
    "        \n",
    "        while self.val_stop == False and self.epoch_stop == False:\n",
    "\n",
    "            print(\"Epoch: \", self.epoch_count+1)\n",
    "            \n",
    "            self.begin_epoch()\n",
    "            \n",
    "            last_loss = self.train_one_epoch(network, train_loader)\n",
    "            self.train_losses.append(last_loss)\n",
    "            last_vloss = self.val_one_epoch(network, val_loader)\n",
    "            self.val_losses.append(last_vloss)\n",
    "            self.val_early_stopping(last_vloss, network, val_patience)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "run_manager = RunManager(epochs)\n",
    "run_manager.weights_init()\n",
    "\n",
    "run_manager.train_val_run(reconstructor, train_loader, val_loader)\n",
    "\n",
    "t_loss = run_manager.test_run(reconstructor, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = run_manager.train_losses\n",
    "val_loss = run_manager.val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(reconstructor.state_dict(), r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\Project_git\\Parameters_training_1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_reconstructor = LinearRegression(124, 512).to(device)\n",
    "unt_loss = run_manager.test_run(untrained_reconstructor, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss before training: \", unt_loss, \"Loss after training: \", t_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding from latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud GPU needed\n",
    "\n",
    "outpath=\"im_variations\"\n",
    "scale=3.0\n",
    "h=512\n",
    "w=512\n",
    "n_samples=4\n",
    "precision=\"fp32\"\n",
    "plms=True\n",
    "ddim_steps=50\n",
    "ddim_eta=0.0\n",
    "device_idx=0\n",
    "save=True\n",
    "eval=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Outputs\\Trial1\"\n",
    "\n",
    "if plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "    ddim_eta = 0.0\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "for img in batch:\n",
    "\n",
    "    img = prep_im(img)\n",
    "\n",
    "    x_samples_ddim = sample_model(img, model, sampler, precision, h, w, ddim_steps, n_samples, scale, ddim_eta)\n",
    "  \n",
    "    for x_sample in x_samples_ddim:\n",
    "        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "        filename = os.path.join(sample_path, f\"{base_count:05}.png\")\n",
    "        Image.fromarray(x_sample.astype(np.uint8)).save(filename)\n",
    "        base_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research_1",
   "language": "python",
   "name": "research_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
