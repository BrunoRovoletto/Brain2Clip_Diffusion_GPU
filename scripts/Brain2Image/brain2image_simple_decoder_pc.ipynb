{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_root = r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\"\n",
    "ckpt=r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\\models\\ldm\\stable-diffusion-v1\\sd-clip-vit-l14-img-embed_ema_only.ckpt\"\n",
    "config=r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\\configs\\stable-diffusion\\sd-image-condition-finetune.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import glob\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install cvxpy for L1 norm minimization for PeriodStrength fun (Ramanujan methods)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import torchvision.transforms as T\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from six.moves import xrange\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from ipynb.fs.full.WaveSeparatorCode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport requests\\nfrom transformers import AutoProcessor, CLIPModel\\nimport tensorflow as tf\\n\\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\\n\\n# Get the image features\\nprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\\n\\n\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprint(type(image))\\n\\ninputs = processor(images=image, return_tensors=\"pt\")\\n\\nimage_features = model.get_image_features(**inputs)\\n\\nprint(image_features.shape) # output shape of image features'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "import tensorflow as tf\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "# Get the image features\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(type(image))\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "image_features = model.get_image_features(**inputs)\n",
    "\n",
    "print(image_features.shape) # output shape of image features'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mtype\u001b[39m(image_features\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_features' is not defined"
     ]
    }
   ],
   "source": [
    "type(image_features.to('cuda'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = (r\"D:\\For Thesis Project\\Datasets\\My_ImageNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class ImportImagenet(Dataset):\\n    def __init__ (self, root_dir, mode, model, processor):\\n        self.root_dir = root_dir\\n        self.device = device\\n        downed_image= None\\n        if mode == \\'Test\\':\\n            fileid = \\'Test_list.csv\\'\\n        elif mode == \\'Val\\':\\n            fileid = \\'Val_list.csv\\'\\n        elif mode == \\'Train\\':\\n            fileid = \\'Train_list.csv\\'\\n        self.csv = pd.read_csv(os.path.join(self.root_dir,fileid), delimiter= \\',\\')       \\n        \\n        self.targets = torch.cuda.IntTensor(self.csv[\"Target\"]).long()\\n        \\n\\n    def __len__(self):\\n        return len(self.csv)\\n\\n    def __getitem__(self,index):\\n\\n        CLIP = True\\n\\n        \\n        img_path = os.path.join(self.root_dir, self.csv.iloc[index,0]).replace(\"\\\\\",\"/\")\\n        image = Image.open(img_path)\\n        \\n        #This is used when NOT preprocessing images for CLIP\\n        if(not CLIP):\\n            \\n            image = T.ToTensor()(image).to(\\'cuda\\')\\n            image = T.CenterCrop((200, 200))(image)\\n\\n            return(image.to(device)) \\n\\n        else:\\n\\n            with torch.no_grad():\\n\\n                inputs = processor(images=image, return_tensors=\"pt\")\\n\\n                image_features = model.get_image_features(**inputs)\\n        \\n\\n            return(inputs[\"pixel_values\"].to(device), image_features.to(device))\\n\\n        \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class ImportImagenet(Dataset):\n",
    "    def __init__ (self, root_dir, mode, model, processor):\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        downed_image= None\n",
    "        if mode == 'Test':\n",
    "            fileid = 'Test_list.csv'\n",
    "        elif mode == 'Val':\n",
    "            fileid = 'Val_list.csv'\n",
    "        elif mode == 'Train':\n",
    "            fileid = 'Train_list.csv'\n",
    "        self.csv = pd.read_csv(os.path.join(self.root_dir,fileid), delimiter= ',')       \n",
    "        \n",
    "        self.targets = torch.cuda.IntTensor(self.csv[\"Target\"]).long()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        CLIP = True\n",
    "\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[index,0]).replace(\"\\\\\",\"/\")\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        #This is used when NOT preprocessing images for CLIP\n",
    "        if(not CLIP):\n",
    "            \n",
    "            image = T.ToTensor()(image).to('cuda')\n",
    "            image = T.CenterCrop((200, 200))(image)\n",
    "\n",
    "            return(image.to(device)) \n",
    "\n",
    "        else:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "        \n",
    "\n",
    "            return(inputs[\"pixel_values\"].to(device), image_features.to(device))\n",
    "\n",
    "        \n",
    "'''\n",
    "         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportImagenet(Dataset):\n",
    "    def __init__ (self, root_dir, mode):\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        downed_image= None\n",
    "        if mode == 'Test':\n",
    "            fileid = 'Test_list.csv'\n",
    "        elif mode == 'Val':\n",
    "            fileid = 'Val_list.csv'\n",
    "        elif mode == 'Train':\n",
    "            fileid = 'Train_list.csv'\n",
    "        self.csv = pd.read_csv(os.path.join(self.root_dir,fileid), delimiter= ',')       \n",
    "        self.targets = torch.cuda.IntTensor(self.csv[\"Target\"]).long()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        CLIP = True\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[index,0])\n",
    "\n",
    "     \n",
    "            \n",
    "        \n",
    "        #This is used when NOT preprocessing images for CLIP\n",
    "\n",
    "        image = io.imread(img_path)\n",
    "        # y_label = self.targets[index]\n",
    "        image = T.ToTensor()(image)\n",
    "        image = T.CenterCrop((200, 200))(image)\n",
    "        image = T.Resize(192)(image)\n",
    "\n",
    "\n",
    "\n",
    "        # Tokenizing with Openclip, and feeding it to the dataloader as labels\n",
    "        _image = Image.open(img_path)\n",
    "        CLIP_image = preprocess(_image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            image_token = model.encode_image(CLIP_image.to(device).squeeze(1)).float()\n",
    "            image_token = image_token.squeeze(1)\n",
    "\n",
    "        if(CLIP == True):\n",
    "\n",
    "            return(CLIP_image.to(device), image_token.to(device))\n",
    "\n",
    "        else:\n",
    "            return(image.to(device), image_token.to(device))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.transpose(inputs[\"pixel_values\"].squeeze(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "subset = True\n",
    "subset_cut = 10\n",
    "shuffle = True\n",
    "\n",
    "\n",
    "\n",
    "# data_set = ImportImagenet(root_dir, 'Train', model, processor)\n",
    "data_set = ImportImagenet(root_dir, 'Train')\n",
    "dataset_len= len(data_set)\n",
    "\n",
    "if (subset == True):\n",
    "\n",
    "   \n",
    "    # Subset dataset\n",
    "    data_set = torch.utils.data.Subset(data_set, np.arange(0,len(data_set),subset_cut))\n",
    "    \n",
    "    dataset_len = len(data_set)\n",
    "    \n",
    "    val_split = int(0.1 * dataset_len)\n",
    "    test_split = int(0.1 * dataset_len)\n",
    "    train_split = dataset_len - val_split - test_split\n",
    "\n",
    "else:\n",
    "    \n",
    "    val_split = int(0.1 * dataset_len)\n",
    "    test_split = int(0.1 * dataset_len)\n",
    "    train_split = dataset_len - val_split - test_split\n",
    "\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data_set, [train_split, val_split, test_split])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)\n",
    "test_loader = torch.utils.data.DataLoader(train_set, batch_size=batch , shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.566389GB\n",
      "torch.cuda.memory_reserved: 0.619141GB\n",
      "torch.cuda.max_memory_reserved: 0.619141GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimages = next(iter(train_loader))\\n\\nfor image in images[0].squeeze(dim=1):\\n   \\n    plt.imshow(T.ToPILImage()(image.squeeze()))\\n    plt.figure()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize images (they appear distorted when processed for clip)\n",
    "'''\n",
    "images = next(iter(train_loader))\n",
    "\n",
    "for image in images[0].squeeze(dim=1):\n",
    "   \n",
    "    plt.imshow(T.ToPILImage()(image.squeeze()))\n",
    "    plt.figure()'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for a single electrode (linear regression)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 124 electrodes\n",
    "\n",
    "num_electrodes = 124\n",
    "# input_size = 224*224*3 # Flattened image size\n",
    "input_size = 150528\n",
    "output_size = 1\n",
    "electrodes = nn.ModuleList([LinearRegression(input_size, output_size).to(device) for _ in range(num_electrodes)])\n",
    "\n",
    "# Pass the image through each electrode and collect the outputs\n",
    "\n",
    "def readBrain(images, electrodes = electrodes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        brain_data = []\n",
    "        for image in images: \n",
    "            electrode_data = []\n",
    "            for electrode in electrodes:\n",
    "                \n",
    "                electrode_data.append(electrode(image.flatten()))\n",
    "\n",
    "            # Concatenate the outputs into a single tensor\n",
    "            electrodes_data = torch.cat(electrode_data, dim=0)\n",
    "\n",
    "            brain_data.append(electrodes_data)\n",
    "          \n",
    "\n",
    "        brain_data = torch.stack(brain_data, dim=0)\n",
    "\n",
    "        n_brain_data = brain_data + (0.3)*torch.randn(brain_data.shape).to(device)\n",
    "    \n",
    "        return n_brain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBrain_nobatch(image, electrodes = electrodes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        brain_data = []\n",
    "        \n",
    "        electrode_data = []\n",
    "        for electrode in electrodes:\n",
    "      \n",
    "            electrode_data.append(electrode(image.flatten()))\n",
    "\n",
    "        # Concatenate the outputs into a single tensor\n",
    "        electrodes_data = torch.cat(electrode_data, dim=0)\n",
    "\n",
    "        brain_data.append(electrodes_data)\n",
    "          \n",
    "\n",
    "        brain_data = torch.stack(brain_data, dim=0)\n",
    "\n",
    "        n_brain_data = brain_data + (0.3)*torch.randn(brain_data.shape).to(device)\n",
    "    \n",
    "        return n_brain_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing Latents from simulated electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructor = LinearRegression(124, 512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "\n",
    "    def __init__(self, max_epoch, learning_rate = 0.01, network = reconstructor):\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.best_vloss = None\n",
    "        self.bad_validation_counter = 0\n",
    "        self.val_stop = False\n",
    "        self.saved_parameters1 = None\n",
    "        self.max_epoch = max_epoch\n",
    "        self.epoch_stop = False\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.Adam(reconstructor.parameters(), lr=learning_rate)\n",
    "        self.network = network\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for layer in self.network.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "\n",
    "    def val_early_stopping(self,  vloss, network, patience):\n",
    "    \n",
    "\n",
    "        if self.best_vloss == None:\n",
    "            self.best_vloss = vloss\n",
    "\n",
    "        elif self.best_vloss > vloss:      #if best_vloss > vloss  \n",
    "            self.bad_validation_counter = 0\n",
    "            self.best_vloss = vloss\n",
    "            \n",
    "        elif self.best_vloss <= vloss:      #if best_vloss <= vloss\n",
    "            self.bad_validation_counter += 1\n",
    "            \n",
    "            if self.bad_validation_counter == 1:\n",
    "                self.saved_parameters1 = network.state_dict() \n",
    "                        \n",
    "# Stop if validation performance does not improve for patience number of epochs\n",
    "        if self.bad_validation_counter >= patience:\n",
    "            self.val_stop = True\n",
    "            print(\"Val stop\")\n",
    "\n",
    "    def train_track_loss(self, loss, batch):\n",
    "        self.train_epoch_loss += loss.item()\n",
    "        \n",
    "    def val_track_loss(self, loss, batch):\n",
    "        self.val_epoch_loss += loss.item()\n",
    "\n",
    "    def test_track_loss(self, loss, batch):\n",
    "        self.test_epoch_loss += loss.item()\n",
    "\n",
    "    def begin_epoch(self):  \n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.train_epoch_loss = 0\n",
    "        self.test_epoch_loss = 0\n",
    "        self.val_epoch_loss = 0\n",
    "\n",
    "    # Stop if max_epoch is reached\n",
    "        if self.max_epoch != None:\n",
    "            if self.epoch_count == self.max_epoch:\n",
    "                self.epoch_stop = True\n",
    "\n",
    "\n",
    "    def networkStep(self, network, loader, mode):\n",
    "\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in loader:\n",
    "\n",
    "                n_batches += 1\n",
    "\n",
    "                images, tokens = batch\n",
    "      \n",
    "                brain_data = readBrain(images)\n",
    "                if(mode == \"Train\"):\n",
    "                    self.optimizer.zero_grad();\n",
    "                output = network(brain_data)\n",
    "                loss = self.loss_function(output, tokens.squeeze())\n",
    "\n",
    "                if(mode == \"Train\"):\n",
    "             \n",
    "                    loss.backward();\n",
    "                    self.optimizer.step();\n",
    "                 \n",
    "                    self.train_track_loss(loss,batch)\n",
    "\n",
    "                elif(mode == \"Val\"):\n",
    "                  \n",
    "                    self.val_track_loss(loss,batch)\n",
    "                else:\n",
    "                    self.test_track_loss(loss,batch)\n",
    "                    \n",
    "                    \n",
    "\n",
    "        return n_batches\n",
    "\n",
    "    def train_one_epoch(self, network, train_loader):\n",
    "\n",
    "        network.train()\n",
    "        last_loss = 0         \n",
    "        n_batches = self.networkStep(network, train_loader, \"Train\")                  \n",
    "        last_loss = self.train_epoch_loss / n_batches \n",
    "        \n",
    "        return last_loss\n",
    "\n",
    "    def val_one_epoch(self, network, val_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_vloss = 0\n",
    "            n_batches = self.networkStep(network, val_loader, \"Val\")     \n",
    "            last_vloss = self.val_epoch_loss / n_batches \n",
    "    \n",
    "        return last_vloss\n",
    "\n",
    "    def test_run(self, network, test_loader):\n",
    "        self.begin_epoch()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_tloss = 0\n",
    "            n_batches = self.networkStep(network, test_loader, \"Test\")\n",
    "            last_tloss = self.test_epoch_loss / n_batches \n",
    "\n",
    "        return last_tloss\n",
    "    \n",
    "    \n",
    "    def train_val_run(self, network, train_loader, val_loader, val_patience = 3):\n",
    "        \n",
    "        while self.val_stop == False and self.epoch_stop == False:\n",
    "\n",
    "            print(\"Epoch: \", self.epoch_count+1)\n",
    "            \n",
    "            self.begin_epoch()\n",
    "            \n",
    "            last_loss = self.train_one_epoch(network, train_loader)\n",
    "            self.train_losses.append(last_loss)\n",
    "            last_vloss = self.val_one_epoch(network, val_loader)\n",
    "            self.val_losses.append(last_vloss)\n",
    "            self.val_early_stopping(last_vloss, network, val_patience)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n",
      "Epoch:  10\n",
      "Epoch:  11\n",
      "Epoch:  12\n",
      "Epoch:  13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>run_manager.weights_init()                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>6 run_manager.train_val_run(reconstructor, train_loader, val_loader)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8 </span>t_loss = run_manager.test_run(reconstructor, test_loader)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">9 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_val_run</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   │   </span>last_loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.train_one_epoch(network, train_loader)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.train_losses.append(last_loss)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>141 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>last_vloss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.val_one_epoch(network, val_loader)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.val_losses.append(last_vloss)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.val_early_stopping(last_vloss, network, val_patience)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">val_one_epoch</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   │   │   </span>network.eval()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   │   │   </span>last_vloss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>n_batches = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.networkStep(network, val_loader, <span style=\"color: #808000; text-decoration-color: #808000\">\"Val\"</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   │   │   </span>last_vloss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.val_epoch_loss / n_batches                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> last_vloss                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">networkStep</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 72 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 │   │   │   │   </span>n_batches = n_batches + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 75 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>images, tokens = batch                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   │   │   </span>brain_data = readBrain(images)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 78 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span>(mode == <span style=\"color: #808000; text-decoration-color: #808000\">\"Train\"</span>):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>too many values to unpack <span style=\"font-weight: bold\">(</span>expected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mrun_manager.weights_init()                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m6 run_manager.train_val_run(reconstructor, train_loader, val_loader)                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m8 \u001b[0mt_loss = run_manager.test_run(reconstructor, test_loader)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m9 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtrain_val_run\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   \u001b[0mlast_loss = \u001b[96mself\u001b[0m.train_one_epoch(network, train_loader)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.train_losses.append(last_loss)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m141 \u001b[2m│   │   │   \u001b[0mlast_vloss = \u001b[96mself\u001b[0m.val_one_epoch(network, val_loader)                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.val_losses.append(last_vloss)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.val_early_stopping(last_vloss, network, val_patience)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mval_one_epoch\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mnetwork.eval()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   │   \u001b[0mlast_vloss = \u001b[94m0\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   │   \u001b[0mn_batches = \u001b[96mself\u001b[0m.networkStep(network, val_loader, \u001b[33m\"\u001b[0m\u001b[33mVal\u001b[0m\u001b[33m\"\u001b[0m)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   │   \u001b[0mlast_vloss = \u001b[96mself\u001b[0m.val_epoch_loss / n_batches                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m last_vloss                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mnetworkStep\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 72 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mn_batches = n_batches + \u001b[94m1\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 75 \u001b[2m│   │   │   │   \u001b[0mimages, tokens = batch                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbrain_data = readBrain(images)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 78 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m(mode == \u001b[33m\"\u001b[0m\u001b[33mTrain\u001b[0m\u001b[33m\"\u001b[0m):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mtoo many values to unpack \u001b[1m(\u001b[0mexpected \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1000\n",
    "run_manager = RunManager(epochs)\n",
    "\n",
    "run_manager.weights_init()\n",
    "\n",
    "run_manager.train_val_run(reconstructor, train_loader, val_loader)\n",
    "\n",
    "t_loss = run_manager.test_run(reconstructor, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = run_manager.train_losses\n",
    "val_loss = run_manager.val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.3975,  0.4413,  0.4705,  ...,  0.4267,  0.4267,  0.3975],\n",
      "           [ 0.3683,  0.4121,  0.4559,  ...,  0.4267,  0.4851,  0.4997],\n",
      "           [ 0.3537,  0.3975,  0.4267,  ...,  0.5143,  0.4851,  0.4997],\n",
      "           ...,\n",
      "           [ 0.1493,  0.1347,  0.1347,  ...,  0.1785,  0.1639,  0.1639],\n",
      "           [ 0.1785,  0.1347,  0.1347,  ...,  0.1785,  0.1493,  0.1639],\n",
      "           [ 0.1493,  0.1493,  0.1347,  ...,  0.1639,  0.1493,  0.1347]],\n",
      "\n",
      "          [[ 0.5291,  0.5741,  0.6041,  ...,  0.5291,  0.5741,  0.5891],\n",
      "           [ 0.4991,  0.5291,  0.5591,  ...,  0.5741,  0.6341,  0.6191],\n",
      "           [ 0.4691,  0.5141,  0.5291,  ...,  0.5891,  0.6491,  0.6642],\n",
      "           ...,\n",
      "           [ 0.1839,  0.1839,  0.1839,  ...,  0.1989,  0.2139,  0.2139],\n",
      "           [ 0.2289,  0.1839,  0.1989,  ...,  0.2139,  0.2139,  0.2139],\n",
      "           [ 0.1989,  0.1839,  0.1989,  ...,  0.1989,  0.2139,  0.1989]],\n",
      "\n",
      "          [[ 0.6101,  0.6670,  0.6812,  ...,  0.6244,  0.6386,  0.6386],\n",
      "           [ 0.5959,  0.6244,  0.6670,  ...,  0.6386,  0.6955,  0.6955],\n",
      "           [ 0.5817,  0.5817,  0.5959,  ...,  0.6812,  0.7097,  0.7381],\n",
      "           ...,\n",
      "           [ 0.3257,  0.2546,  0.2404,  ...,  0.3115,  0.2973,  0.3115],\n",
      "           [ 0.3257,  0.2831,  0.2831,  ...,  0.2973,  0.2688,  0.2831],\n",
      "           [ 0.2831,  0.2973,  0.2973,  ...,  0.2973,  0.2973,  0.2973]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157],\n",
      "           [ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157],\n",
      "           [ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157],\n",
      "           ...,\n",
      "           [ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157],\n",
      "           [ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157],\n",
      "           [ 1.9157,  1.9157,  1.9157,  ...,  1.9157,  1.9157,  1.9157]],\n",
      "\n",
      "          [[ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599],\n",
      "           [ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599],\n",
      "           [ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599],\n",
      "           ...,\n",
      "           [ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599],\n",
      "           [ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599],\n",
      "           [ 2.0599,  2.0599,  2.0599,  ...,  2.0599,  2.0599,  2.0599]],\n",
      "\n",
      "          [[ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "           [ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "           [ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "           ...,\n",
      "           [ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "           [ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "           [ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           ...,\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303]],\n",
      "\n",
      "          [[ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           ...,\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749]],\n",
      "\n",
      "          [[ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           ...,\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.2515,  0.2661,  0.1931,  ...,  0.4121,  0.4413,  0.4997],\n",
      "           [ 0.3975,  0.1785, -0.0259,  ...,  0.3099,  0.3391,  0.4705],\n",
      "           [ 0.0617, -0.0550, -0.3178,  ...,  0.6311,  0.4851,  0.3829],\n",
      "           ...,\n",
      "           [-0.3324, -0.3616, -0.3762,  ..., -0.5514, -0.8142, -0.9164],\n",
      "           [-0.3908, -0.4054, -0.3324,  ..., -0.6390, -0.7558, -0.7558],\n",
      "           [-0.1864, -0.1718, -0.3178,  ..., -0.7996, -0.8142, -0.7558]],\n",
      "\n",
      "          [[ 0.3190,  0.3190,  0.2589,  ...,  0.6341,  0.6041,  0.6491],\n",
      "           [ 0.4691,  0.2589,  0.0789,  ...,  0.5141,  0.4841,  0.6041],\n",
      "           [ 0.1689,  0.0638, -0.2213,  ...,  0.8593,  0.6642,  0.5441],\n",
      "           ...,\n",
      "           [-0.1313, -0.1613, -0.1763,  ..., -0.2063, -0.4464, -0.5365],\n",
      "           [-0.0862, -0.1313, -0.1012,  ..., -0.3414, -0.4464, -0.4614],\n",
      "           [ 0.0188,  0.0188, -0.1313,  ..., -0.4164, -0.4614, -0.4314]],\n",
      "\n",
      "          [[ 0.1977,  0.1977,  0.1266,  ...,  0.6386,  0.5817,  0.6528],\n",
      "           [ 0.3399,  0.1409, -0.0582,  ...,  0.5106,  0.4821,  0.6101],\n",
      "           [ 0.0555, -0.0724, -0.3426,  ...,  0.8377,  0.6386,  0.5390],\n",
      "           ...,\n",
      "           [-0.2289, -0.2715, -0.2573,  ..., -0.2289, -0.4706, -0.5701],\n",
      "           [-0.1151, -0.1862, -0.1720,  ..., -0.3568, -0.4848, -0.5133],\n",
      "           [-0.0724, -0.0867, -0.2289,  ..., -0.4990, -0.5417, -0.4990]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           ...,\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "           [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303]],\n",
      "\n",
      "          [[ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           ...,\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "           [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749]],\n",
      "\n",
      "          [[ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           ...,\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "           [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.5435,  0.5873,  0.6457,  ..., -0.1426,  0.5143,  0.6019],\n",
      "           [ 0.6019,  0.6311,  0.6895,  ...,  0.0033,  0.5727,  0.5873],\n",
      "           [ 0.6895,  0.6749,  0.6749,  ...,  0.3245,  0.5289,  0.6019],\n",
      "           ...,\n",
      "           [-0.6974, -0.7266, -0.7996,  ..., -1.7923, -1.7923, -1.7923],\n",
      "           [-0.7558, -0.7704, -0.7996,  ..., -1.7923, -1.7923, -1.7923],\n",
      "           [-0.7850, -0.7850, -0.7850,  ..., -1.7923, -1.7923, -1.7923]],\n",
      "\n",
      "          [[-0.0112, -0.0112, -0.0262,  ..., -0.5815,  0.4090,  0.5291],\n",
      "           [ 0.0038,  0.0188,  0.0638,  ..., -0.1012,  0.4240,  0.4691],\n",
      "           [ 0.0789,  0.0789,  0.1089,  ...,  0.1839,  0.4991,  0.4691],\n",
      "           ...,\n",
      "           [-1.6470, -1.6170, -1.5570,  ..., -1.7521, -1.7521, -1.7521],\n",
      "           [-1.6470, -1.6621, -1.6771,  ..., -1.7521, -1.7521, -1.7521],\n",
      "           [-1.6320, -1.6921, -1.7071,  ..., -1.7521, -1.7521, -1.7521]],\n",
      "\n",
      "          [[-0.5986, -0.6270, -0.6555,  ..., -0.3568,  0.5248,  0.5817],\n",
      "           [-0.4990, -0.5275, -0.5133,  ...,  0.0413,  0.5532,  0.5390],\n",
      "           [-0.3853, -0.4279, -0.4279,  ...,  0.3684,  0.5817,  0.5248],\n",
      "           ...,\n",
      "           [-1.4660, -1.4660, -1.4376,  ..., -1.4802, -1.4802, -1.4802],\n",
      "           [-1.4376, -1.4660, -1.4660,  ..., -1.4802, -1.4802, -1.4802],\n",
      "           [-1.4233, -1.4518, -1.4091,  ..., -1.4802, -1.4802, -1.4802]]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reconstructor.state_dict(), r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\\scripts\\Brain2Image\\saved_checkpoints\\small_clip_linear\\small_clip_linear_subset_10_b_64_epochs_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructor.load_state_dict(torch.load(r\"C:\\Users\\bruno\\OneDrive\\Desktop\\BrainReader RESEARCH\\Code\\external_gits\\StableDiffusion_img2img\\stable-diffusion\\scripts\\Brain2Image\\saved_checkpoints\\small_clip_linear\\small_clip_linear_subset_10_b_64_epochs_11\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m untrained_reconstructor \u001b[39m=\u001b[39m LinearRegression(\u001b[39m124\u001b[39m, \u001b[39m512\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m unt_loss \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39;49mtest_run(untrained_reconstructor, test_loader)\n\u001b[0;32m      3\u001b[0m t_loss \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mtest_run(reconstructor, test_loader)\n",
      "Cell \u001b[1;32mIn[23], line 124\u001b[0m, in \u001b[0;36mRunManagerDec.test_run\u001b[1;34m(self, network, test_loader)\u001b[0m\n\u001b[0;32m    122\u001b[0m     network\u001b[39m.\u001b[39meval()\n\u001b[0;32m    123\u001b[0m     last_tloss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 124\u001b[0m     n_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetworkStep(network, test_loader, \u001b[39m\"\u001b[39;49m\u001b[39mTest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    125\u001b[0m     last_tloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_epoch_loss \u001b[39m/\u001b[39m n_batches \n\u001b[0;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m last_tloss\n",
      "Cell \u001b[1;32mIn[23], line 75\u001b[0m, in \u001b[0;36mRunManagerDec.networkStep\u001b[1;34m(self, network, loader, mode)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m     73\u001b[0m         n_batches \u001b[39m=\u001b[39m n_batches \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 75\u001b[0m         images, tokens \u001b[39m=\u001b[39m batch\n\u001b[0;32m     77\u001b[0m         \u001b[39mif\u001b[39;00m(mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     78\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad();\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "untrained_reconstructor = LinearRegression(124, 512).to(device)\n",
    "unt_loss = run_manager.test_run(untrained_reconstructor, test_loader)\n",
    "t_loss = run_manager.test_run(reconstructor, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training:  0.5405282754729432 Loss after training:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss before training: \", unt_loss, \"Loss after training: \", train_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding from latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class Pixelnorm(nn.Module):\n",
    "    def __init__(self, epsilon: float = 1e-8) -> None:\n",
    "        super(Pixelnorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(torch.square(x), dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class Bias(nn.Module):\n",
    "    def __init__(self, shape: tuple) -> None:\n",
    "        super(Bias, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.bias = nn.Parameter(torch.zeros(*shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, channels: int, in_channels: int) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.in_channels = in_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Pixelnorm(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Pixelnorm()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.repeat_interleave(x, 2, dim=2)\n",
    "        x = torch.repeat_interleave(x, 2, dim=3)\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            Pixelnorm(),\n",
    "            nn.Linear(512, 8192, bias=False),\n",
    "            Bias((512,)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Pixelnorm(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Pixelnorm(),\n",
    "            Block(512, 512),  # 8\n",
    "            Block(512, 512),\n",
    "            Block(512, 512),\n",
    "            Block(256, 512),\n",
    "            Block(128, 256),\n",
    "            Block(64, 128),\n",
    "            Block(32, 64),\n",
    "            Block(16, 32),  # 15\n",
    "            nn.Conv2d(16, 3, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, layer):\n",
    "        x = self.generator[1](self.generator[0](x))\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        for i in range(2, len(self.generator)):\n",
    "            x = self.generator[i](x)\n",
    "            if i == layer + 7:\n",
    "                print(x.shape)\n",
    "                return x\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManagerDec():\n",
    "\n",
    "    def __init__(self, max_epoch, generator, learning_rate = 0.01):\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.best_vloss = None\n",
    "        self.bad_validation_counter = 0\n",
    "        self.val_stop = False\n",
    "        self.saved_parameters1 = None\n",
    "        self.max_epoch = max_epoch\n",
    "        self.epoch_stop = False\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = optim.Adam(reconstructor.parameters(), lr=learning_rate)\n",
    "        self.network = generator\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for layer in self.network.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "\n",
    "    def val_early_stopping(self,  vloss, network, patience):\n",
    "    \n",
    "\n",
    "        if self.best_vloss == None:\n",
    "            self.best_vloss = vloss\n",
    "\n",
    "        elif self.best_vloss > vloss:      #if best_vloss > vloss  \n",
    "            self.bad_validation_counter = 0\n",
    "            self.best_vloss = vloss\n",
    "            \n",
    "        elif self.best_vloss <= vloss:      #if best_vloss <= vloss\n",
    "            self.bad_validation_counter += 1\n",
    "            \n",
    "            if self.bad_validation_counter == 1:\n",
    "                self.saved_parameters1 = network.state_dict() \n",
    "                        \n",
    "# Stop if validation performance does not improve for patience number of epochs\n",
    "        if self.bad_validation_counter >= patience:\n",
    "            self.val_stop = True\n",
    "            print(\"Val stop\")\n",
    "\n",
    "    def train_track_loss(self, loss, batch):\n",
    "        self.train_epoch_loss += loss.item()\n",
    "        \n",
    "    def val_track_loss(self, loss, batch):\n",
    "        self.val_epoch_loss += loss.item()\n",
    "\n",
    "    def test_track_loss(self, loss, batch):\n",
    "        self.test_epoch_loss += loss.item()\n",
    "\n",
    "    def begin_epoch(self):  \n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.train_epoch_loss = 0\n",
    "        self.test_epoch_loss = 0\n",
    "        self.val_epoch_loss = 0\n",
    "\n",
    "    # Stop if max_epoch is reached\n",
    "        if self.max_epoch != None:\n",
    "            if self.epoch_count == self.max_epoch:\n",
    "                self.epoch_stop = True\n",
    "\n",
    "\n",
    "    def networkStep(self, network, loader, mode):\n",
    "\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in loader:\n",
    "\n",
    "                n_batches = n_batches + 1\n",
    "\n",
    "                images, tokens = batch\n",
    "      \n",
    "                if(mode == \"Train\"):\n",
    "                    self.optimizer.zero_grad();\n",
    "                output = network(tokens, 9)\n",
    "                loss = self.loss_function(output, images)\n",
    "\n",
    "                if(mode == \"Train\"):\n",
    "             \n",
    "                    loss.backward();\n",
    "                    self.optimizer.step();\n",
    "                 \n",
    "                    self.train_track_loss(loss,batch)\n",
    "\n",
    "                elif(mode == \"Val\"):\n",
    "                  \n",
    "                    self.val_track_loss(loss,batch)\n",
    "                else:\n",
    "                    self.test_track_loss(loss,batch)\n",
    "                    \n",
    "                    \n",
    "\n",
    "        return n_batches\n",
    "\n",
    "    def train_one_epoch(self, network, train_loader):\n",
    "\n",
    "        network.train()\n",
    "        last_loss = 0         \n",
    "        n_batches = self.networkStep(network, train_loader, \"Train\")                  \n",
    "        last_loss = self.train_epoch_loss / n_batches \n",
    "        \n",
    "        return last_loss\n",
    "\n",
    "    def val_one_epoch(self, network, val_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_vloss = 0\n",
    "            n_batches = self.networkStep(network, val_loader, \"Val\")     \n",
    "            last_vloss = self.val_epoch_loss / n_batches \n",
    "    \n",
    "        return last_vloss\n",
    "\n",
    "    def test_run(self, network, test_loader):\n",
    "        self.begin_epoch()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            network.eval()\n",
    "            last_tloss = 0\n",
    "            n_batches = self.networkStep(network, test_loader, \"Test\")\n",
    "            last_tloss = self.test_epoch_loss / n_batches \n",
    "\n",
    "        return last_tloss\n",
    "    \n",
    "    \n",
    "    def train_val_run(self, network, train_loader, val_loader, val_patience = 3):\n",
    "        \n",
    "        while self.val_stop == False and self.epoch_stop == False:\n",
    "\n",
    "            print(\"Epoch: \", self.epoch_count+1)\n",
    "            \n",
    "            self.begin_epoch()\n",
    "            \n",
    "            last_loss = self.train_one_epoch(network, train_loader)\n",
    "            self.train_losses.append(last_loss)\n",
    "            last_vloss = self.val_one_epoch(network, val_loader)\n",
    "            self.val_losses.append(last_vloss)\n",
    "            self.val_early_stopping(last_vloss, network, val_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 2.88 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m run_manager \u001b[39m=\u001b[39m RunManagerDec(epochs, generator)\n\u001b[0;32m      4\u001b[0m run_manager\u001b[39m.\u001b[39mweights_init()\n\u001b[1;32m----> 6\u001b[0m run_manager\u001b[39m.\u001b[39;49mtrain_val_run(generator, train_loader, val_loader)\n\u001b[0;32m      8\u001b[0m t_loss \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mtest_run(generator, test_loader)\n",
      "Cell \u001b[1;32mIn[23], line 138\u001b[0m, in \u001b[0;36mRunManagerDec.train_val_run\u001b[1;34m(self, network, train_loader, val_loader, val_patience)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_count\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin_epoch()\n\u001b[1;32m--> 138\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(network, train_loader)\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses\u001b[39m.\u001b[39mappend(last_loss)\n\u001b[0;32m    140\u001b[0m last_vloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_one_epoch(network, val_loader)\n",
      "Cell \u001b[1;32mIn[23], line 103\u001b[0m, in \u001b[0;36mRunManagerDec.train_one_epoch\u001b[1;34m(self, network, train_loader)\u001b[0m\n\u001b[0;32m    101\u001b[0m network\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    102\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m         \n\u001b[1;32m--> 103\u001b[0m n_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetworkStep(network, train_loader, \u001b[39m\"\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)                  \n\u001b[0;32m    104\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch_loss \u001b[39m/\u001b[39m n_batches \n\u001b[0;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m last_loss\n",
      "Cell \u001b[1;32mIn[23], line 79\u001b[0m, in \u001b[0;36mRunManagerDec.networkStep\u001b[1;34m(self, network, loader, mode)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m(mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad();\n\u001b[1;32m---> 79\u001b[0m output \u001b[39m=\u001b[39m network(tokens, \u001b[39m9\u001b[39;49m)\n\u001b[0;32m     80\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(output, images)\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m(mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 77\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x, layer)\u001b[0m\n\u001b[0;32m     75\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator)):\n\u001b[1;32m---> 77\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator[i](x)\n\u001b[0;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m layer \u001b[39m+\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 46\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(x, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     45\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(x, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(x)\n\u001b[0;32m     47\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m, in \u001b[0;36mPixelnorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39;49msquare(x), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 2.88 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "run_manager = RunManagerDec(epochs, generator)\n",
    "\n",
    "run_manager.weights_init()\n",
    "\n",
    "run_manager.train_val_run(generator, train_loader, val_loader)\n",
    "\n",
    "t_loss = run_manager.test_run(generator, test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research_1",
   "language": "python",
   "name": "research_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
